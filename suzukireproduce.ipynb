{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO8N3QzwDn+QlwDUrDzjQDl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FrozenPeak0701/MusicCopilotGPT/blob/main/suzukireproduce.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi -L\n",
        "!lscpu |grep 'Model name'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UJJQO0cdS1wx",
        "outputId": "5ce19c76-236e-41ac-a61b-42c101e7725e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU 0: Tesla T4 (UUID: GPU-c78fbd4c-82c3-4d48-bf5b-c2699d3184fb)\n",
            "Model name:                      Intel(R) Xeon(R) CPU @ 2.20GHz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "12NRurxaRiV5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86b6034d-1667-4bb2-d6cb-1e0d657c4b0f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "## Mount Google Drive Data (If using Google Colaboratory)\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/gdrive')\n",
        "except:\n",
        "    print(\"Mounting Failed.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls gdrive/MyDrive/Data/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4LtbJ25CS2-i",
        "outputId": "bc533d80-2eae-4506-c85b-0f8e5623a619"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ATEPP-1.1      detokenizer  requirements.txt\t   tokenizer\n",
            "convert.json   musicXml     sample.py\t\t   tokenProduction.ipynb\n",
            "CreateCsv.py   mxl\t    test.ipynb\n",
            "CSV\t       __pycache__  tokenized_files\n",
            "dataloader.py  remi\t    tokenized_scores.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = \"gdrive/MyDrive/Data/\""
      ],
      "metadata": {
        "id": "aYMRbZPS1aNa"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r \"{data_dir}requirements.txt\"\n",
        "!pip install miditoolkit"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gMU8EoLr2Hwm",
        "outputId": "14e478db-b410-4333-8389-1ed206364900"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from -r gdrive/MyDrive/Data/requirements.txt (line 1)) (4.11.2)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from -r gdrive/MyDrive/Data/requirements.txt (line 2)) (4.9.3)\n",
            "Requirement already satisfied: music21 in /usr/local/lib/python3.10/dist-packages (from -r gdrive/MyDrive/Data/requirements.txt (line 3)) (8.1.0)\n",
            "Collecting pretty_midi (from -r gdrive/MyDrive/Data/requirements.txt (line 4))\n",
            "  Downloading pretty_midi-0.2.10.tar.gz (5.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m51.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->-r gdrive/MyDrive/Data/requirements.txt (line 1)) (2.4.1)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.10/dist-packages (from music21->-r gdrive/MyDrive/Data/requirements.txt (line 3)) (4.0.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from music21->-r gdrive/MyDrive/Data/requirements.txt (line 3)) (1.3.1)\n",
            "Requirement already satisfied: jsonpickle in /usr/local/lib/python3.10/dist-packages (from music21->-r gdrive/MyDrive/Data/requirements.txt (line 3)) (3.0.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from music21->-r gdrive/MyDrive/Data/requirements.txt (line 3)) (3.7.1)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from music21->-r gdrive/MyDrive/Data/requirements.txt (line 3)) (9.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from music21->-r gdrive/MyDrive/Data/requirements.txt (line 3)) (1.22.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from music21->-r gdrive/MyDrive/Data/requirements.txt (line 3)) (2.27.1)\n",
            "Requirement already satisfied: webcolors>=1.5 in /usr/local/lib/python3.10/dist-packages (from music21->-r gdrive/MyDrive/Data/requirements.txt (line 3)) (1.13)\n",
            "Collecting mido>=1.1.16 (from pretty_midi->-r gdrive/MyDrive/Data/requirements.txt (line 4))\n",
            "  Downloading mido-1.3.0-py3-none-any.whl (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.3/50.3 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from pretty_midi->-r gdrive/MyDrive/Data/requirements.txt (line 4)) (1.16.0)\n",
            "Requirement already satisfied: packaging~=23.1 in /usr/local/lib/python3.10/dist-packages (from mido>=1.1.16->pretty_midi->-r gdrive/MyDrive/Data/requirements.txt (line 4)) (23.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->music21->-r gdrive/MyDrive/Data/requirements.txt (line 3)) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->music21->-r gdrive/MyDrive/Data/requirements.txt (line 3)) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->music21->-r gdrive/MyDrive/Data/requirements.txt (line 3)) (4.41.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->music21->-r gdrive/MyDrive/Data/requirements.txt (line 3)) (1.4.4)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->music21->-r gdrive/MyDrive/Data/requirements.txt (line 3)) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->music21->-r gdrive/MyDrive/Data/requirements.txt (line 3)) (3.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->music21->-r gdrive/MyDrive/Data/requirements.txt (line 3)) (2.8.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->music21->-r gdrive/MyDrive/Data/requirements.txt (line 3)) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->music21->-r gdrive/MyDrive/Data/requirements.txt (line 3)) (2023.7.22)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->music21->-r gdrive/MyDrive/Data/requirements.txt (line 3)) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->music21->-r gdrive/MyDrive/Data/requirements.txt (line 3)) (3.4)\n",
            "Building wheels for collected packages: pretty_midi\n",
            "  Building wheel for pretty_midi (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pretty_midi: filename=pretty_midi-0.2.10-py3-none-any.whl size=5592287 sha256=e25cf2fecc44aff41a3953e8ac757ca28aecde8023f9922fc6986486dfc2419d\n",
            "  Stored in directory: /root/.cache/pip/wheels/cd/a5/30/7b8b7f58709f5150f67f98fde4b891ebf0be9ef07a8af49f25\n",
            "Successfully built pretty_midi\n",
            "Installing collected packages: mido, pretty_midi\n",
            "Successfully installed mido-1.3.0 pretty_midi-0.2.10\n",
            "Collecting miditoolkit\n",
            "  Downloading miditoolkit-0.1.16-py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: numpy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from miditoolkit) (1.22.4)\n",
            "Requirement already satisfied: mido>=1.1.16 in /usr/local/lib/python3.10/dist-packages (from miditoolkit) (1.3.0)\n",
            "Requirement already satisfied: packaging~=23.1 in /usr/local/lib/python3.10/dist-packages (from mido>=1.1.16->miditoolkit) (23.1)\n",
            "Installing collected packages: miditoolkit\n",
            "Successfully installed miditoolkit-0.1.16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append(data_dir)"
      ],
      "metadata": {
        "id": "vfaf6M7mAgDo"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from dataloader import *\n",
        "import pandas as pd\n",
        "from remi import *\n",
        "import miditoolkit\n",
        "from tokenizer.score_to_tokens import *\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import json\n",
        "import pickle\n",
        "from torch.utils.data import Subset\n",
        "from torch.optim import Adam\n",
        "from torch.nn.utils.rnn import pad_sequence"
      ],
      "metadata": {
        "id": "T407wV47ALpQ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.memory_allocated()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gTAjVM97eT2-",
        "outputId": "c880c187-dae6-4574-804b-617fcf28cff4"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_file_dir = 'tokenized_files/'\n",
        "dataset = CustomTokenDataset(f'{data_dir}CSV/data.csv')\n",
        "tmp = dataset[0]\n",
        "\n",
        "indices = list(range(len(dataset)))\n",
        "td = [310, 308, 307, 306, 305, 304, 282, 280, 279, 278, 276, 264, 263, 256, 245, 242, 220, 213, 212, 211, 209, 207, 203, 196, 194, 193, 192, 191, 190, 186, 184, 182, 181 ,178, 177, 175, 174, 169, 66, 48, 33, 28, 27, 24]\n",
        "for i in td:\n",
        "  del indices[i]\n",
        "\n",
        "new_dataset = Subset(dataset, indices)\n",
        "len(new_dataset)"
      ],
      "metadata": {
        "id": "ACqU5MhCZQ-v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "223faa32-ccba-4769-c5d4-e1f46196918d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "275"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenized_scores_list = ['sos', 'eos']\n",
        "# max_midi_token_len = 0\n",
        "# max_score_token_len = 0\n",
        "# files = []\n",
        "# for i,dat in enumerate(new_dataset):\n",
        "#   print(f\"{i + len(td)}|\", end = '')\n",
        "#   try:\n",
        "#     midi_obj = miditoolkit.midi.parser.MidiFile(data_dir+dat[2])\n",
        "#     if len(midi_obj.instruments) == 2:\n",
        "#       if (midi_obj.instruments[0].name!=\"Piano\") or ((midi_obj.instruments[1].name!=\"Piano\")):\n",
        "#         print(f\"{i}th object's midi file isn't 2 pianos: {midi_obj.instruments[0].name}, {midi_obj.instruments[1].name}\")\n",
        "#       else:\n",
        "#         print(f\"{i}th object's midi file is 2 pianos\")\n",
        "#     elif len(midi_obj.instruments) == 1:\n",
        "#       print(f\"{i}th object's midi file is 1 piano: {midi_obj.instruments[0].name}\")\n",
        "#     else:\n",
        "#       print(f\"{i}th object's midi file is more than two piano: {[i.name for i in midi_obj.instruments]}\")\n",
        "#     max_midi_token_len = max(*[len(i.notes) for i in midi_obj.instruments], max_midi_token_len)\n",
        "#     scoretokens = MusicXML_to_tokens(data_dir+dat[1])\n",
        "#     for tokj in scoretokens:\n",
        "#         if (tokj not in tokenized_scores_list):\n",
        "#           tokenized_scores_list.append(tokj)\n",
        "#     max_score_token_len = max(max_score_token_len, len(scoretokens))\n",
        "#     files.append(dat[0])\n",
        "#   except (TypeError, FileNotFoundError, AttributeError) as e:\n",
        "#     print(f\"{e}\")\n",
        "# with open(data_dir+tokenized_file_dir+'correspondance.json', 'w') as f:\n",
        "#     json.dump(files, f)\n",
        "# tokenized_scores_list, max_score_token_len, max_midi_token_len"
      ],
      "metadata": {
        "id": "LNE_mjQDdqjJ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# with open(data_dir+'tokenized_scores.json', 'w') as f:\n",
        "#     json.dump(tokenized_scores_list, f)\n",
        "\n",
        "with open(data_dir+'tokenized_scores.json') as f:\n",
        "    tokenized_scores_list = json.load(f)"
      ],
      "metadata": {
        "id": "lziahqS48zrp"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(tokenized_scores_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VclpDenxCBCB",
        "outputId": "c5be5a1e-0f9d-420a-81c1-bf7608cd2780"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "422"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize(value, length):\n",
        "    return value / length\n",
        "\n",
        "def encode(note, song_length):\n",
        "    start_token = int(normalize(note.start, song_length) * 100)\n",
        "    end_token = int(normalize(note.end, song_length) * 100)\n",
        "    pitch_token = note.pitch\n",
        "    velocity_token = note.velocity\n",
        "    return [start_token, end_token, pitch_token, velocity_token]"
      ],
      "metadata": {
        "id": "Fdk75HmcdrJu"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for i, dat in enumerate(new_dataset):\n",
        "#   song = miditoolkit.midi.parser.MidiFile(data_dir+dat[2])\n",
        "#   song_length = song.max_tick\n",
        "#   dnotes = sorted([j for i in song.instruments for j in i.notes], key=lambda note: note.start)\n",
        "#   tokens = [encode(note, song_length) for note in dnotes ]\n",
        "#   with open(data_dir + tokenized_file_dir+f'midi{i}.pkl', 'wb') as f:\n",
        "#     pickle.dump(tokens, f)"
      ],
      "metadata": {
        "id": "DAqd1FDC0tdU"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# with open(data_dir + tokenized_file_dir+'midi1.pkl', 'rb') as f:\n",
        "#     tokens = pickle.load(f)\n",
        "# tokens"
      ],
      "metadata": {
        "id": "p_K62L2i8YJz"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a mapping from tokens to integers\n",
        "token_to_int = {token: i for i, token in enumerate(tokenized_scores_list)}\n",
        "\n",
        "# Create a mapping from integers to tokens\n",
        "int_to_token = {i: token for token, i in token_to_int.items()}\n",
        "\n",
        "def token2intlist(tokens):\n",
        "  return [token_to_int[i] for i in tokens]\n",
        "def int2tokenlist(ints):\n",
        "  return [int_to_token[i] for i in ints]"
      ],
      "metadata": {
        "id": "QlyJ6ltT_2GV"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for i, dat in enumerate(new_dataset):\n",
        "#   strlist = MusicXML_to_tokens(data_dir+dat[1])\n",
        "#   strlist = ['sos',] + strlist + ['eos',]\n",
        "#   tokens = token2intlist(strlist)\n",
        "#   with open(data_dir + tokenized_file_dir+f'score{i}.pkl', 'wb') as f:\n",
        "#     pickle.dump(tokens, f)"
      ],
      "metadata": {
        "id": "fy4ojiw29vOc"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# with open(data_dir + tokenized_file_dir+f'score0.pkl', 'rb') as f:\n",
        "#     tokens = pickle.load(f)\n",
        "# tokens"
      ],
      "metadata": {
        "id": "DFI79zRV9-Ik"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "srcr = []\n",
        "tgtr = []\n",
        "max_src_len = 0\n",
        "max_tgt_len = 0\n",
        "for i in range(275):\n",
        "  with open(data_dir + tokenized_file_dir+f'score{i}.pkl', 'rb') as f:\n",
        "      tok = pickle.load(f)\n",
        "      max_tgt_len = max(max_tgt_len, len(tok))\n",
        "      tgtr.append(tok)\n",
        "\n",
        "  with open(data_dir + tokenized_file_dir+f'midi{i}.pkl', 'rb') as f:\n",
        "      tok = pickle.load(f)\n",
        "      max_src_len = max(max_src_len, len(tok))\n",
        "      srcr.append(tok)\n",
        "len(srcr), len(tgtr), max_src_len, max_tgt_len"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qrjuu-FgKJ-Q",
        "outputId": "c72aa69a-44c8-4ec7-c072-7cf4a3c02e17"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(275, 275, 10188, 35382)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(tgtr[0]), len(srcr[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pxGD6CsVS_oU",
        "outputId": "84059a73-9364-4461-a388-064f4d030904"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4976, 1288)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MusicDataset(Dataset):\n",
        "    def __init__(self, src_data, tgt_data):\n",
        "        self.src_data = src_data\n",
        "        self.tgt_data = tgt_data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.src_data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.src_data[idx], self.tgt_data[idx]\n",
        "\n",
        "def pad_sequences(batch, padding_value=-1, feature_size = None):\n",
        "    # Get sequence lengths\n",
        "    lengths = [len(seq) for seq in batch]\n",
        "\n",
        "    # Create an empty tensor with size of the longest sequence\n",
        "    max_length = max(lengths)\n",
        "    if feature_size is None:\n",
        "      padded_batch = torch.full((len(batch), max_length), padding_value)\n",
        "    else:\n",
        "      padded_batch = torch.full((len(batch), max_length, feature_size), padding_value)\n",
        "\n",
        "    # Pad the sequences\n",
        "    for i, seq in enumerate(batch):\n",
        "        padded_batch[i, :lengths[i]] = seq\n",
        "\n",
        "    return padded_batch\n",
        "\n",
        "def collate_fn(batch):\n",
        "    src_batch, tgt_batch = zip(*batch)\n",
        "\n",
        "    src_batch = pad_sequences([torch.tensor(data) for data in src_batch],feature_size = 4)\n",
        "    tgt_batch = pad_sequences([torch.tensor(data) for data in tgt_batch])\n",
        "\n",
        "    return src_batch, tgt_batch\n",
        "\n",
        "# Assuming your data is stored in src_data and tgt_data\n",
        "dataset = MusicDataset(srcr, tgtr)\n",
        "\n",
        "# Create a DataLoader\n",
        "dataloader = DataLoader(dataset, batch_size=2, collate_fn=collate_fn)  #, shuffle = True"
      ],
      "metadata": {
        "id": "8lVK_MiWJ0q2"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, embedding_size, num_heads, hidden_dim, num_layers, dropout, device, max_src_length=11000, max_tgt_length = 36000):\n",
        "        super().__init__()\n",
        "\n",
        "        self.device = device\n",
        "\n",
        "        self.src_linear = nn.Linear(4, embedding_size)\n",
        "        self.tgt_linear = nn.Linear(1, embedding_size)\n",
        "        self.src_pos_embedding = nn.Embedding(max_src_length, embedding_size)\n",
        "        self.tgt_pos_embedding = nn.Embedding(max_tgt_length, embedding_size)\n",
        "\n",
        "        self.encoder = nn.TransformerEncoder(\n",
        "            nn.TransformerEncoderLayer(embedding_size, num_heads, hidden_dim, dropout),\n",
        "            num_layers\n",
        "        )\n",
        "\n",
        "        self.decoder = nn.TransformerDecoder(\n",
        "            nn.TransformerDecoderLayer(embedding_size, num_heads, hidden_dim, dropout),\n",
        "            num_layers\n",
        "        )\n",
        "\n",
        "        self.fc = nn.Linear(embedding_size, 1)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def create_mask(self, src, tgt):\n",
        "      src_mask = (src.sum(dim=-1) == -4)\n",
        "      tgt_mask = (tgt == -1)\n",
        "      print(src_mask,tgt_mask, sep = '\\n')\n",
        "      # # This line ensures that the `inf` values do not result in NaN values after softmax\n",
        "      # src_mask = src_mask.masked_fill(src_mask, sys.float_info.min)\n",
        "      # tgt_mask = tgt_mask.masked_fill(tgt_mask, sys.float_info.min)\n",
        "      # print(src_mask,tgt_mask, sep ='\\n')\n",
        "      return src_mask, tgt_mask\n",
        "\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "      print(1,torch.cuda.memory_allocated()*1e-9)\n",
        "      src = src.float()\n",
        "      tgt = tgt.float()\n",
        "\n",
        "      batch_size, src_seq_len, _ = src.size()\n",
        "      _, tgt_seq_len = tgt.size()  # get sequence length of tgt\n",
        "      print(2,torch.cuda.memory_allocated()*1e-9)\n",
        "      src_mask, tgt_mask = self.create_mask(src, tgt)\n",
        "\n",
        "      print(src_mask.shape, tgt_mask.shape,torch.cuda.memory_allocated()*1e-9)\n",
        "\n",
        "      pos = (torch.arange(0, src_seq_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)).long()\n",
        "      tgt_pos = (torch.arange(0, tgt_seq_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)).long()  # pos for tgt\n",
        "      print(pos.shape, tgt_pos.shape)\n",
        "      print(3,torch.cuda.memory_allocated()*1e-9)\n",
        "      # print((self.src_linear(src) + self.src_pos_embedding(pos)).shape, (self.tgt_linear(tgt.unsqueeze(-1)) + self.tgt_pos_embedding(tgt_pos)).shape)\n",
        "      src_embedded = self.dropout((self.src_linear(src) + self.src_pos_embedding(pos)).permute(1,0,2))\n",
        "      tgt_embedded = self.dropout((self.tgt_linear(tgt.unsqueeze(-1)) + self.tgt_pos_embedding(tgt_pos)).permute(1,0,2))\n",
        "\n",
        "      print(4,torch.cuda.memory_allocated()*1e-9)\n",
        "      print(src_mask.shape, tgt_mask.shape, src_embedded.shape, tgt_embedded.shape)\n",
        "      enc_src = self.encoder(src_embedded, src_key_padding_mask=src_mask)\n",
        "      print(5,torch.cuda.memory_allocated()*1e-9)\n",
        "      # print(src_mask, tgt_mask, src_mask.shape, tgt_mask.shape)\n",
        "      print(enc_src.shape)\n",
        "      output = self.decoder(tgt_embedded, enc_src, tgt_key_padding_mask=tgt_mask, memory_key_padding_mask=src_mask)\n",
        "      print(6,torch.cuda.memory_allocated()*1e-9)\n",
        "      return self.fc(output.permute(1, 0, 2))"
      ],
      "metadata": {
        "id": "qrgcfZWGuQUW"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize model\n",
        "embedding_size = 64\n",
        "num_heads = 4\n",
        "hidden_dim = 64\n",
        "num_layers = 2\n",
        "dropout = 0.2\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = Transformer(embedding_size, num_heads, hidden_dim, num_layers, dropout, device).to(device)\n",
        "\n",
        "# Initialize optimizer and loss function\n",
        "learning_rate = 0.0001\n",
        "optimizer = Adam(model.parameters(), lr=learning_rate)\n",
        "criterion = nn.MSELoss()"
      ],
      "metadata": {
        "id": "oufxxgYuuN6L"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2C5giQsFMA2r",
        "outputId": "5e1dce8c-1085-45b2-934e-fb0ecfdbaa49"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Transformer(\n",
              "  (src_linear): Linear(in_features=4, out_features=64, bias=True)\n",
              "  (tgt_linear): Linear(in_features=1, out_features=64, bias=True)\n",
              "  (src_pos_embedding): Embedding(11000, 64)\n",
              "  (tgt_pos_embedding): Embedding(36000, 64)\n",
              "  (encoder): TransformerEncoder(\n",
              "    (layers): ModuleList(\n",
              "      (0-1): 2 x TransformerEncoderLayer(\n",
              "        (self_attn): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
              "        )\n",
              "        (linear1): Linear(in_features=64, out_features=64, bias=True)\n",
              "        (dropout): Dropout(p=0.2, inplace=False)\n",
              "        (linear2): Linear(in_features=64, out_features=64, bias=True)\n",
              "        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "        (dropout1): Dropout(p=0.2, inplace=False)\n",
              "        (dropout2): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (decoder): TransformerDecoder(\n",
              "    (layers): ModuleList(\n",
              "      (0-1): 2 x TransformerDecoderLayer(\n",
              "        (self_attn): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
              "        )\n",
              "        (multihead_attn): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
              "        )\n",
              "        (linear1): Linear(in_features=64, out_features=64, bias=True)\n",
              "        (dropout): Dropout(p=0.2, inplace=False)\n",
              "        (linear2): Linear(in_features=64, out_features=64, bias=True)\n",
              "        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "        (norm3): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "        (dropout1): Dropout(p=0.2, inplace=False)\n",
              "        (dropout2): Dropout(p=0.2, inplace=False)\n",
              "        (dropout3): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (fc): Linear(in_features=64, out_features=1, bias=True)\n",
              "  (dropout): Dropout(p=0.2, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w_KMi0_HMEiA",
        "outputId": "bcbfcdd8-a5e7-4074-8e9e-56ddef144ea3"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The model has 3,142,913 trainable parameters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (src, tgt) in enumerate(dataloader):\n",
        "      # if i==0:\n",
        "        # Send your batch to the correct device\n",
        "        # torch.cuda.empty_cache()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        src = src.to(device).float()\n",
        "        tgt = tgt.to(device).float()\n",
        "\n",
        "        print('src shape and tgt shape:',src.shape,tgt.shape)\n",
        "        # Pass data through model\n",
        "        # The input to the model is the source sequences only\n",
        "        output = model(src, tgt)\n",
        "\n",
        "        # print(output, output.shape, tgt.shape)\n",
        "        # Compute loss\n",
        "        loss = criterion(output.squeeze(-1), tgt)\n",
        "        print(7, torch.cuda.memory_allocated()*1e-9)\n",
        "        # Backward pass and optimization\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if i % 1 == 0:\n",
        "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(dataloader)}], Loss: {loss.detach().item()}')\n",
        "\n",
        "# Save model\n",
        "# torch.save(model.state_dict(), \"transformer_model.pt\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 768
        },
        "id": "maKoBIAF22-a",
        "outputId": "3e460293-be40-4732-c832-ddc27444a9ce"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "src shape and tgt shape: torch.Size([2, 2930, 4]) torch.Size([2, 7942])\n",
            "1 0.012741632000000001\n",
            "2 0.012741632000000001\n",
            "tensor([[False, False, False,  ...,  True,  True,  True],\n",
            "        [False, False, False,  ..., False, False, False]], device='cuda:0')\n",
            "tensor([[False, False, False,  ...,  True,  True,  True],\n",
            "        [False, False, False,  ..., False, False, False]], device='cuda:0')\n",
            "torch.Size([2, 2930]) torch.Size([2, 7942]) 0.01276416\n",
            "torch.Size([2, 2930]) torch.Size([2, 7942])\n",
            "3 0.012938752000000001\n",
            "4 0.028417024000000003\n",
            "torch.Size([2, 2930]) torch.Size([2, 7942]) torch.Size([2930, 2, 64]) torch.Size([7942, 2, 64])\n",
            "5 1.3055129600000002\n",
            "torch.Size([2930, 2, 64])\n",
            "6 13.891348480000001\n",
            "7 13.885887488000002\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-0cda8160f1d4>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;31m# Backward pass and optimization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    485\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    488\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 712.00 MiB (GPU 0; 14.75 GiB total capacity; 12.92 GiB already allocated; 462.81 MiB free; 13.52 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nzdvkJ-Eng41"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}